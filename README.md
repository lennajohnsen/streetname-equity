# Instagram Filter

![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589214630197_mockup-01.png)


**PROJECT DESCRIPTION:**
*Streetname Equity* is an Instagram filter designed to disrupt the patriarchal nature of our built environment, allow users to discover womxn who lived in / worked in / shaped Cambridge, and ask questions about who is (and isn’t) commemorated in the landscape.

This document is intended to familiarize readers with Instagram Stories and Filters, outline how to use *Streetname Equity*, describe and document our workflow and process, hopefully allowing for replication, and point to resources for the further development of the project.


**WHY INSTAGRAM:**
Because the primary audience of *Women Take the Streets* is students and teachers in Cambridge schools, we sought to utilize the popularity of social media platforms such as Instagram and Snapchat to develop an engagement tool that could be used outside of the classroom as well as by members of the broader Cambridge community. 

While it was not possible within the scope of the semester to build and deploy filters for both Instagram and Snapchat, we acknowledge that a Snapchat filter would expand our audience to pre-teens as well as teenagers. We ultimately chose to develop an Instagram filter because of our own familiarity with the platform and its language, as well as its broader popularity with young adults and the existing presence on Instagram of both the Cambridge Historical Commission and the Women's Heritage Project.


**HOW AN INSTAGRAM FILTER WORKS:**
An Instagram filter allows users to overlay other images and 3D objects onto their photos and videos. While for Instagram Posts, this primarily involves editing the images by adjusting the colors, brightness, contrast etc. similar to Photoshop, Instagram has developed more robust and interactive editing capabilities for the Stories platform that involve text, stickers, animations, sounds, and 3D effects.  

![Source](https://i2.wp.com/insta-go.it/wp-content/uploads/2018/10/InstagramStories-blog-MAIN810x440.jpg?fit=810%2C440&ssl=1&resize=1200%2C900)


Instagram Stories are photos and videos that disappear after 24 hours on the platform, as opposed to Posts that stay on a user's profile. Because of this, stories are often more lighthearted and personal in nature as they are windows into users’ everyday lives. They appear along the top of the Instagram interface, as depicted at left. To watch a Story, a user taps on one of the circles. Stories are broadcasted in a reel format, meaning that users can watch all the stories of all the people they follow in one go. 

One recent trend on Instagram is the "What ____ are you?" Filter, some examples of which can be seen below. These range in topic from "What dog breed are you?" to "Which Pokemon are you?" to "What NYC subway line are you?." A clear extension of Millennial’s obsession with Buzzfeed quizzes ([example](https://www.buzzfeed.com/ashleyperez/what-city-should-you-actually-live-in) here) that purport to glean insight into one's personality and future from a series of multiple choice questions, Instagram filters of this format are incredibly popular. 


![](https://lh3.googleusercontent.com/PpjoV6S7rdhiPmhpJjNs91GU4MKZc5_ZY7CMwVSWZeKfSjtlbXOXfZ5S8Cece01vYBi1qIO5WHY_AErR5vIq5ewkI1WL1AVtkNi8QkypVSWhvsz0IUEhtu31fOXeTRhgmJQDCsB0mKA)
![](https://lh6.googleusercontent.com/_I6clIFRzB4ce_fpzMYciaCD3cLR9YO5cvfIe78oUG1g7HLKwbEVIbqqScL2xC4cRsXhLw71l9FwxNtkeLwXHluj6JJSS8QN5wmjOQXE6-6qtv9um1XN4ExaWxo1Ub0ttkzfA62sND4)


Users find these Filters principally through other user’s Stories, meaning they are extremely viral in nature. For example, if one of the accounts I follow posts a video using the "What dog breed are you?" Filter shown above, I would use the link in the upper left to try it on my own camera. If I were to then share the footage of me discovering that I am, in fact, a shiba inu at heart to my Story, my followers could then do the same, which could in turn inspire *their* followers and so on. 

These filters are fairly standard in their visual language and user interactions-- upon opening the filter, a box appears above a user's head, and when "record" is pressed and held, slides of options begin to rapidly cycle, eventually landing on one option. While Buzzfeed quizzes ostensibly have some sort of algorithm to match users’ responses to results, Instagram Filters of this ilk are completely random. 

However, brands and organizations have recently begun exploring more environment-based Instagram Filters. For example, National Geographic launched a Filter that uses their iconic yellow-bordered magazine as a trigger to start the application. Considering the broader trend to use AR to engage with the built environment, we wanted to test the capabilities of an Instagram Filter to disrupt the everyday nature of the streets in Cambridge.


**HOW TO USE** ***STREETNAME EQUITY*****:**
These instructions assume you have an Instagram account, and the application installed on your phone. There is a way to get to the Filter through my personal Instagram account, because 

![](https://lh6.googleusercontent.com/1rDpWqHHkq1yWyGD3Akk1pc1x5txHVQFG-RMKSOGomlIkZLurclR44DhqwE9jG7wlWVUVsC47Qb9HEGxh4HZxjyt7lw3IaVtPsKskQ3qQYIs8iWe54o_Y90K-fajxXvcnViCNAB2IMM)



1. Open up your phone’s camera and scan the QR code at right OR use this link on your phone: https://instagram.com/a/r/?effect_id=243090973699626 


2. Either method will prompt you to open the Instagram phone application (this won't work on your computer) and will pre-load the Filter into your camera.


![](https://lh3.googleusercontent.com/GeDfa4mVNyz1pKci48y5UTqHe6E4pnbzISnVxTCPRZ0vBKj-JbdaoJOCoW3cFj3nRjHYOvzIE17A7WhnNoK2Wpv2joMs3wJfaAU-5iWVKe9x5gEeoIFh6iLFOGz9q3uN8tVjcRrTFfU)

3. At this point, your screen should display something like the image on the right. Ideally you are standing on the sidewalk facing a street, but the Filter’s plane detection works in any context, meaning you could rename your laptop, your dining room table, your floor.


4. You can move the street sign by dragging it around the screen and also scale it by using the pinching motion. Do this to set the 3D sign into the desired position. If you want, you can tap it to set it into place but this isn't required.


5. Once you have it positioned as you like, press and hold record to watch the animation cycle through the womxn we have uploaded to the Filter until it randomly selects one. This should take 6 seconds in total.
    

A design decision was made to replicate the visual language and user interface of the Filters described in the section above above — the rapid cycling before a random option is chosen. This is partly to enable Instagram users to already be familiar with how the Filter will work (there are limits on the amount of instructions one is allowed to include — more on that below). It was also decided that the result should be random to speak to the seemingly arbitrary nature of street naming. Except in cases where streets are commemorated in honor of someone who lived on the street, or someone who did something important on that street, the question of "why this person at this time in this place" remains.

Ideally, what this Filter does is prompt the users to consider more carefully the process of street naming and spark curiosity about who is represented in the built environment. Some of what we initially wanted to accomplish with the Filter, specifically more pointed calls to action, were complicated by the Review Policies associated with the platform. As any user can upload a Filter, there is a review process and a set of rules that all content must follow. All these rules can be found here, at [Review Policies](https://sparkar.facebook.com/ar-studio/learn/documentation/before-you-start/policy/), but these are the most relevant to our project:


- **2.4. Visual Text**
    - **2.4.b.** Text must not be excessive – no more than one sentence.
    - **2.4.c.** Text must not prompt people to navigate away from the effect or off the Instagram or Facebook platforms.
- **2.7. Photographs.** Effects must not contain photographs of people, whether real or fictional.
- **4.6. External links and tags.** Content must not contain tags or links off the Facebook or Instagram platforms.
    - **4.6.a.** Content must not contain:
        - hashtags
        - URLs (web addresses)
        - QR codes
        - other scannable codes

 
These rules impacted our design in a few ways. Firstly, it was decided that the biographical information was the most important to include for the user experience. This then took up our one sentence of text allotted, per 2.4.b. Filters can't prompt people to move away from the effect, per 2.4.c., nor can tags/links to off platform sites be included, per 4.6. which meant that we couldn't include a link to the project website, for example. One potential way around this would be to link to an Instagram account (not off the platform) that would have more information. It would then depend on the Reviewer to determine if that created "excessive text." 

Secondly, it meant that the portraits of womxn had to be abstracted from photographs, which in the end, added to the design of the Filter and allows for a potential link to the cards (not possible within the timeline of this project, but for future iterations should this process be repeated). The portraits used in the filter can be seen below. These were created from photos found online, then manipulated in Photoshop and Illustrator. The files are included in this Dropbox folder.


![](https://lh3.googleusercontent.com/vExiHL0megqQxvzR5fqbB5e7hkfiXLHR-fpq7svEZezwPyTkLCQtD_Y5qHmNEwQLupcSVUzaj2_q7dHa-xOU1WNO_-ygEJbmZhIxoz3SBUk819nozS3Yvk_7hyHce_CeFk8El6jLxZ8)


**HOW AN INSTAGRAM FILTER IS MADE:**
While Instagram itself provides some Filters, for example ones that add sunglasses to your face, apply tiger facepaint, turn your background into a tropical scene, most Filters are uploaded by individual users through [Facebook’s Spark AR platform](https://sparkar.facebook.com/augmented-reality/). Spark AR is a free Augmented Reality creation software that allows users to "create interactive AR experiences, with or without code, at any level of expertise." It was opened to the public in August of 2019, and provides thorough documentation as well as sample project files and tutorials that mean that the simplest of Filters could be deployed within hours. Every project created within the platform has access to libraries that enable face detection, target tracking, plane tracking, and hand detection. These allow applications to be controlled by facial expression, for example using an eyebrow raise to trigger an animation. A screenshot of the introductory interface can be seen below. 


![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589230129598_image.png)


**PLATFORM INTERFACE:**
Shown below is the final project within the SparkAR interface. What follows is first a description and then instructions on how to update the filter.

![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589232202130_image.png)


The Layers panel is located at top left and beneath it is the Assets panel, which is where the above components are imported. 

At bottom center is the Patch Editor, where the components are controlled using patches. This essentially is an abstracted version of Javascript — in fact, SparkAR allows you to write everything in JS as opposed to using these patches, which are considered more user-friendly with a lower barrier to entry. However, there is a limit of one JS file per Filter, and the decision was made to learn the SparkAR, which hopefully makes it more replicable, so this Filter is built using patches. 

Above this is the View container that previews the Filter and allows you to position the imported assets. There are different backgrounds one can choose for the mockup, here showing an iPhone8, including different people you can test the facial detection on. 

However, *Streetname Equity* is built using the *Plane Tracker* capability, documented [here](https://sparkar.facebook.com/ar-studio/learn/documentation/docs/plane-tracker/), which detects a horizontal plane in the environment to trigger the Filter. At this moment, vertical planes can't be detected within the capabilities of the platform, which is why the Filter wasn't designed to detect a street sign, but rather a horizontal plane on which to place our model of a street sign. 


![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589232548115_image.png)


What you see at left are the component parts of the Filter: the plane tracker that detects the horizontal plane (planeTracker0), which has as children the 3D model of the sign (Mesh1 Model) which in turn has as children the 2D planes that display the prompt and the womxn options.  These parent/child relationships are important.

As noted above, the 2D prompt and womxn options were created in Adobe Illustrator. These all need to be the exact same size to allow for smooth cycling within the Filter.

The 3D model of the sign is also included in the project folder, but was created in Trimble Sketchup and exported as a .obj. This also could have been done in any number of 3D modeling software, and SketchAR also has a warehouse of models available for download. For the purposes of this project, the decision was made to create this ourselves. The sign itself has very low detail resolution so as to not distract from the surroundings or compete with the information on the womxn.

In addition, the sign has a rotating ring beneath it that is meant to signal to the user that it can be moved-- manip_ring, which is controlled by the looping animation. This, however, was an asset included in a tutorial offered by SparkAR. 


![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589233852934_image.png)


At left is the Assets panel, which is where these components were imported before being added to the scene. These assets are then mapped to the components placed in the scene, which are then controlled by the patches. 

In short the component *2D plane* *"answers"* is built from the *material*  "responses," which uses the a*nimation sequence* "responses," which is made from the *texture sequence* "StreetNames-Stylized[1-11]." **In an ideal world, I would have named all of these the same thing, but here we are.

The Assets you have to touch (minimally) in order to update the Filter are the *texture sequence* "StreetNames-Stylized-[1-11]" (the womxn), the *material* "responses".

**HOW TO UPDATE:**
To swap out or add womxn to the options, you’ll first want to replicate using the template included in this Dropbox folder, and then Export the artboards as .png (this allows for transparency, which makes the sign look less out of place in the environment).

Once those are exported, you can add them to the Assets panel as *textures* either by dragging them in or by selecting "+ Add Asset," located in the bottom right of the panel. The next 2 steps are shown below.


1. Once they are added to the panel, shift-select all of the *textures* you just imported. Look to the far right of the interface in the properties panel. Change the type to be "Texture Sequence"
2. Then select the *animation sequence* "responses" (located at the bottom of the assets panel). Again, look to the properties panel and change the "Texture" attribute to the texture sequence you created in step 1.

This process changes the material "responses," which then changes the component in the scene, provided you don't change the name of the material. 


![1.](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589234276441_image.png)
![2.](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589234327200_image.png)

![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589235373708_image.png)


To test that it worked, use the "Test on device" option to either send it locally to your phone (connected via USB and with the SparkAR app installed) or by sending a link to your Instagram account. If it works, then use the "Upload" button just below it to send to the Review Process. Depending on the context in which you're updating, you’ll either be making a new Filter tied to your instagram account or updating the one I created.






**THE NUTS AND BOLTS:**

![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589236231755_image.png)


Because the platform is fairly new, there are multiple schools of thought on how to achieve the same result, and because of the recent popularity of "What ____ are you?" filters there are a myriad of instructional videos online ([here](/ep/redirect/external-link?url=https%3A%2F%2Fblog.hootsuite.com%2Finstagram-ar-filters%2F&hmac=GrNO6rHBNjRh3ZTctybD8i8i4NKuACQW%2FjQNedsy2AY%3D), and [here](https://youtu.be/FH_sg17pxMY), for example) showing how to make something similar.  The above is the totality of the Patches used, but I’ll walk through each chunk. For documentation, I’ll provide a brief overview here of what the Filter is doing under the hood. 

If you're updating the Filter, you shouldn't need to touch these unless SparkAR has updated and rendered some of these patches obsolete. You can click to enlarge any of these photos.


![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589235632247_image.png)


This is the sequence of patches that allows the 3D object to be placed-- first, using the Plane Finder to find a horizontal plane, and then enabling Screen Pan to determine "object"’s 3D position. "Object" is what I creatively named the parent component that contains the sign, the prompt, and the responses. Screen Pinch here is enabled to determine the 3D Scale of the same object. The Pack and Unpack patches in between are converting the input into numeric values that can be used by those two variables. 


![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589235880545_image.png)


This sequence is enabling the custom instructions. SparkAR, at this moment, doesn't allow you to insert your own instructions but instead you have to choose from their pre-loaded ones. I could only tell how to put in two. What the above is showing is that when the back or front camera is active, first "Tap to place" is shown for 4 seconds and then "Press to Launch" is shown. This is enabled using the Runtime and then a series of offset and less than patches, followed by a If Then Else to switch between the two.

To add other instruction, navigate to PROJECT/EDIT PROPERTIES/CAPABILITIES to see the below.

![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589235981585_image.png)


And finally, where the animation sequence is controlled. First, this sequence of patches is only activated when the camera starts actively recording (the last node on the purple Camera patch seen at top left). There is a delay function that shows only the prompt for .3 seconds (This could likely be longer, from the feedback we’ve gotten) and then switches to the prompt animation sequence after a pulse delivers a boolean to the runtime offset. The less than+loop animation sequence determines how fast the options are cycled through (this could also be slower). The random and round patches turn this input into a round number that corresponds to one of the responses, which is then shown when the animation hits 5 seconds (determined in the less than patch earlier in the sequence).


![](https://paper-attachments.dropbox.com/s_5E72747BDD6A4708A6792180A6599E8E3946FD069C55D5AF2A898909AE635F26_1589236149540_image.png)




